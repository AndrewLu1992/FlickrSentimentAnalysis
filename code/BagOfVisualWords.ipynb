{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import re \n",
    "import random as rand\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cPickle\n",
    "\n",
    "def get_files(path):\n",
    "    \"\"\" Return a list of file names in this directory that end in .jpg \n",
    "    The list should be sorted alphabetically by file name.\n",
    "    Params:\n",
    "        path....a directory containing .txt review files.\n",
    "    Returns:\n",
    "        a list of .txt file names, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    result = sorted([f for f in os.listdir(path) if f.endswith(\".jpg\")])\n",
    "    result = [path + os.sep + f for f in result]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find all urls in a text file\n",
    "def findurl(filename):\n",
    "    string = io.open(filename, encoding='utf8').readlines()\n",
    "    return [item.strip().split()[1] for item in string]\n",
    "\n",
    "#Retrieve photos in a text file.\n",
    "#Because of large amount of dataset. We randomly retrieve at most 200 photos\n",
    "#in a set.\n",
    "def retrieveImg(path, urls, maxNum=200):\n",
    "    if len(urls) < maxNum:\n",
    "        maxNum = len(urls)\n",
    "    sample = rand.sample(urls, maxNum)\n",
    "    for i in range(0, len(sample)):\n",
    "        urllib.urlretrieve(sample[i], path + os.sep + \"%s.jpg\" % str(i))\n",
    "\n",
    "#Find RNPs.\n",
    "def findLabel(filenames):\n",
    "    return [os.path.splitext(name)[0].split(os.sep)[-1] \\\n",
    "            for name in filenames]\n",
    "\n",
    "#Download images.\n",
    "def download(filenames, ANPs, start, end):\n",
    "    for i in range(start, end):\n",
    "        path = \"data\"+os.sep+ANPs[i]\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        urls = findurl(filenames[i])\n",
    "        retrieveImg(path, urls)\n",
    "        print ANPs[i]+\" finished.\"\n",
    "\n",
    "#path = \"URL\\URL1553\"\n",
    "#all_files = get_files(path)\n",
    "#ANPs = findLabel(all_files) \n",
    "#download(all_files, ANPs, 190, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is to extrct surf descriptor for a given set.\n",
    "def featureExtract(filenames, hessianT=500):\n",
    "    features = []\n",
    "    noneFeatures = []\n",
    "    for name in filenames:\n",
    "        if os.path.getsize(name) > 3000:\n",
    "            img = cv2.imread(name)\n",
    "            surf = cv2.SURF(hessianT)\n",
    "            if len(img.shape) > 2:\n",
    "                gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            kp, des = surf.detectAndCompute(gray,None)\n",
    "            if des is not None:\n",
    "                features.append(des)\n",
    "            else:\n",
    "                noneFeatures.append(name)\n",
    "                os.remove(name)\n",
    "    return np.vstack(tuple(features)), features, noneFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is quantization of feature vectors given a list of features in a set of images.\n",
    "#The same code from BagOfVisualWords.py\n",
    "def quantization(features, codebook, t=0.7):\n",
    "    '''\n",
    "    Input\n",
    "    features --- a list of surf descriptors for each image in dataset\n",
    "    codebook --- visual vocabulary\n",
    "    t --- a threshold control if a match should be included.\n",
    "    Output\n",
    "    an ndarray matrix. Each row represents an image. \n",
    "                        Each column is a visual word.\n",
    "                        An entry in the matrix is the frequency of such visual words in an image.\n",
    "    '''\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    k=0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, {})\n",
    "    result = np.zeros((len(features), len(codebook)), dtype=int)\n",
    "    \n",
    "    for feature in features:\n",
    "        matches = flann.knnMatch(feature, codebook, k=2)\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < t*n.distance:\n",
    "                result[k, m.trainIdx] += 1\n",
    "        k += 1\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is to perform kmeans clustering on surf features.\n",
    "def kmeans(data, k, maxIteration=10, accuracy=1.0, attempts=10):\n",
    "    '''\n",
    "    learning visual vocabulary for each ANP\n",
    "    '''\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + \\\n",
    "                cv2.TERM_CRITERIA_MAX_ITER, maxIteration, accuracy)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    compactness,labels,centers = cv2.kmeans(data, k, criteria, attempts, flags)\n",
    "    return compactness,labels,centers\n",
    "\n",
    "def findK(data):\n",
    "    '''\n",
    "    This is to estimate a possible # of clusters in a dataset.\n",
    "    Also the size of the codebook.\n",
    "    Initial k.\n",
    "    Rule of thumb estimation.\n",
    "    https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\n",
    "    '''\n",
    "    return int((data.shape[0] / 2.0) ** 0.5+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        for c in hash:\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_training_image(path,name):\n",
    "    result = []\n",
    "    result.extend(get_files(path + os.sep + name))\n",
    "    r = repeatable_random(42)\n",
    "    other_dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d)) and d != name]\n",
    "    iters = len(result)\n",
    "    for i in range(0, iters):\n",
    "        j = rand.randint(0,len(other_dirs)-1)\n",
    "        images = get_files(path+os.sep+other_dirs[j])\n",
    "        #print images\n",
    "        result.append(images[rand.randint(0,len(images)-1)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_true_labels(path, name):\n",
    "    \"\"\"Return a *numpy array* of ints for the true sentiment labels of each file.\n",
    "    1 means positive, 0 means negative. Use the name of the file to determine\n",
    "    the true label.\n",
    "    Params:\n",
    "        path....file path\n",
    "        filename....the class that hope to train\n",
    "    Returns:\n",
    "        a numpy array of 1 or 0 values corresponding to each element\n",
    "        of file_names, where 1 indicates a positive review, and 0\n",
    "        indicates a negative review.\n",
    "    \"\"\"\n",
    "    length = len(get_files(path+os.sep+name))         \n",
    "    return np.array([1] * (length) + [0] * (length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X, y, n_folds=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform n-fold cross validation, calling get_clf() to train n\n",
    "    different classifiers. Use sklearn's KFold class: http://goo.gl/wmyFhi\n",
    "    Be sure not to shuffle the data, otherwise your output will differ.\n",
    "    Params:\n",
    "        X.........a csr_matrix of feature vectors\n",
    "        y.........the true labels of each document\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        verbose...If true, report the testing accuracy for each fold.\n",
    "    Return:\n",
    "        the average testing accuracy across all folds.\n",
    "    \"\"\"\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    accuracies = []\n",
    "    counter = 0\n",
    "    for train_idx, test_idx in cv:\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        predicted = clf.predict(X[test_idx])\n",
    "        acc = accuracy_score(y[test_idx], predicted)\n",
    "        if verbose:\n",
    "            print \"fold %d accuracy = %.4f\" %(counter, acc)\n",
    "        counter+=1\n",
    "        accuracies.append(acc)\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_classifiers(path,filenames,codebook):\n",
    "    result = []\n",
    "    for name in filenames:\n",
    "        print name\n",
    "        training_sets = select_training_image(path,name)\n",
    "        features = featureExtract(training_sets)\n",
    "        X = quantization(features[1],codebook,t=1)\n",
    "        y = get_true_labels(path, name)\n",
    "        X, y = repeatable_shuffle(X,y)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(X,y)\n",
    "        result.append(clf)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_codebook(size,filenames):\n",
    "    iters = size\n",
    "    result = []\n",
    "    for i in range(0, iters):\n",
    "        j = rand.randint(0,len(filenames)-1)\n",
    "        images = get_files(path+os.sep+filenames[j])\n",
    "        #print images\n",
    "        result.append(images[rand.randint(0,len(images)-1)])\n",
    "    features = featureExtract(result)\n",
    "    codebook = kmeans(features[0], findK(features[0]))\n",
    "    return codebook[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_class(classifers,path,filenames,codebook,n_eachfile=10):\n",
    "    probs = []\n",
    "    images = []\n",
    "    for i in range(0,len(classifiers)):\n",
    "        images.extend(get_files(path+os.sep+filenames[i])[:n_eachfile])\n",
    "    features = featureExtract(images)\n",
    "    #print len(features[1])\n",
    "    X = quantization(features[1],codebook,t=1)\n",
    "    #print X.shape\n",
    "    for i in range(0,len(classifiers)):\n",
    "        probs.append(classifiers[i].predict_proba(X))\n",
    "    prediction = []\n",
    "    for i in range(0,len(filenames)):\n",
    "        for j in range(0,n_eachfile):\n",
    "            prediction.append({\"files path\":images[i*n_eachfile+j], \n",
    "                               \"true label\":filenames[i],\n",
    "                               \"predict\":filenames[np.argmax([x[i*n_eachfile+j][1] for x in probs])]})\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction,n_eachfile):\n",
    "    for i in range(0,len(prediction)/n_eachfile):\n",
    "        counter = 0\n",
    "        for j in range(0,n_eachfile):\n",
    "            if prediction[i*n_eachfile+j]['true label'] == prediction[i*n_eachfile+j]['predict']:\n",
    "                counter += 1\n",
    "        print \"The accuracy of %s is %f\" %(prediction[i*n_eachfile]['true label'], 1.*counter/n_eachfile)\n",
    "    return 1.*len([item for item in prediction if item['true label'] == item['predict']])/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeEmpty(filenames):\n",
    "    for filename in filenames:\n",
    "        files = get_files(path + os.sep + filename)\n",
    "        for item in files:\n",
    "            if os.path.getsize(item) <= 3000:\n",
    "                os.remove(item)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "filenames = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "with open('labels.pkl', 'wb') as fid:\n",
    "    cPickle.dump(filenames, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#codebook = build_codebook(1250,filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candid_teen\n",
      "charming_city\n",
      "charming_house\n",
      "charming_lady\n",
      "charming_places\n",
      "charming_smile\n",
      "charming_street\n",
      "cheerful_face\n",
      "cheerful_flowers\n",
      "cheerful_smile\n",
      "christian_artist\n",
      "christian_band\n",
      "christian_bible\n",
      "christian_book\n",
      "christian_church\n",
      "christian_concert\n",
      "christian_cross\n",
      "christian_faith\n",
      "christian_festival\n",
      "christian_heritage\n"
     ]
    }
   ],
   "source": [
    "with open('my_dumped_codebook.pkl', 'rb') as fid:\n",
    "    codebook_1 = cPickle.load(fid)\n",
    "classifiers = build_classifiers(path,filenames[180:200],codebook_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "with open('classifier181_200.pkl', 'wb') as fid:\n",
    "    cPickle.dump(classifiers, fid)\n",
    "#with open(\"codebook.pkl\", 'wb') as fid1:\n",
    "    #cPickle.dump(codebook,fid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('classifier101_120.pkl', 'rb') as fid:\n",
    "    classifiers_1 = cPickle.load(fid)\n",
    "len(classifiers_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction =  predict_class(classifiers_1,path,filenames[70:80],codebook_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"the average accuracy is %f\" %(compute_accuracy(prediction,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"adorable_puppy\"\n",
    "training_sets = select_training_image(\"data\",filename)\n",
    "features = featureExtract(training_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#codebook = clustering(200,features[0])[1]\n",
    "X = quantization(features[1], codebook_1,t=1)\n",
    "y = get_true_labels(\"data\",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = repeatable_shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('average cross validation accuracy=%.4f' %\n",
    "      do_cross_validation(X, y,n_folds=5, verbose=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
